{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"%matplotlib inline\n# python libraties\nimport os, cv2,itertools\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nfrom tqdm import tqdm\nfrom glob import glob\nfrom PIL import Image\n\n# pytorch libraries\nimport torch\nfrom torch import optim,nn\nfrom torch.autograd import Variable\nfrom torch.utils.data import DataLoader,Dataset\nfrom torchvision import models,transforms\n\n# sklearn libraries\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report\n\n# to make the results are reproducible\nnp.random.seed(10)\ntorch.manual_seed(10)\ntorch.cuda.manual_seed(10)\n\nprint(os.listdir(\"../input\"))","execution_count":1,"outputs":[{"output_type":"stream","text":"['hmnist_28_28_RGB.csv', 'HAM10000_metadata.csv', 'hmnist_28_28_L.csv', 'ham10000_images_part_2', 'hmnist_8_8_RGB.csv', 'ham10000_images_part_1', 'hmnist_8_8_L.csv']\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"## Data analysis and preprocessing"},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"data_dir = '../input'\nall_image_path = glob(os.path.join(data_dir, '*', '*.jpg'))\nimageid_path_dict = {os.path.splitext(os.path.basename(x))[0]: x for x in all_image_path}\nlesion_type_dict = {\n    'nv': 'Melanocytic nevi',\n    'mel': 'dermatofibroma',\n    'bkl': 'Benign keratosis-like lesions ',\n    'bcc': 'Basal cell carcinoma',\n    'akiec': 'Actinic keratoses',\n    'vasc': 'Vascular lesions',\n    'df': 'Dermatofibroma'\n}","execution_count":2,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This function is used to compute the mean and standard deviation on the whole dataset, will use for inputs normalization"},{"metadata":{"trusted":true},"cell_type":"code","source":"def compute_img_mean_std(image_paths):\n    \"\"\"\n        computing the mean and std of three channel on the whole dataset,\n        first we should normalize the image from 0-255 to 0-1\n    \"\"\"\n\n    img_h, img_w = 224, 224\n    imgs = []\n    means, stdevs = [], []\n\n    for i in tqdm(range(len(image_paths))):\n        img = cv2.imread(image_paths[i])\n        img = cv2.resize(img, (img_h, img_w))\n        imgs.append(img)\n\n    imgs = np.stack(imgs, axis=3)\n    print(imgs.shape)\n\n    imgs = imgs.astype(np.float32) / 255.\n\n    for i in range(3):\n        pixels = imgs[:, :, i, :].ravel()  # resize to one row\n        means.append(np.mean(pixels))\n        stdevs.append(np.std(pixels))\n\n    means.reverse()  # BGR --> RGB\n    stdevs.reverse()\n\n    print(\"normMean = {}\".format(means))\n    print(\"normStd = {}\".format(stdevs))\n    return means,stdevs","execution_count":3,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Return the mean and std of RGB channels"},{"metadata":{"trusted":true},"cell_type":"code","source":"norm_mean,norm_std = compute_img_mean_std(all_image_path)","execution_count":4,"outputs":[{"output_type":"stream","text":"100%|██████████| 10015/10015 [02:09<00:00, 77.12it/s]\n","name":"stderr"},{"output_type":"stream","text":"(224, 224, 3, 10015)\nnormMean = [0.76303977, 0.5456453, 0.5700469]\nnormStd = [0.14092807, 0.15261321, 0.16997075]\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"Add three columns to the original DataFrame, path (image path), cell_type (the whole name),cell_type_idx (the corresponding index  of cell type, as the image label )"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_original = pd.read_csv(os.path.join(data_dir, 'HAM10000_metadata.csv'))\ndf_original['path'] = df_original['image_id'].map(imageid_path_dict.get)\ndf_original['cell_type'] = df_original['dx'].map(lesion_type_dict.get)\ndf_original['cell_type_idx'] = pd.Categorical(df_original['cell_type']).codes\ndf_original.sample(5)","execution_count":5,"outputs":[{"output_type":"execute_result","execution_count":5,"data":{"text/plain":"        lesion_id      image_id   dx    dx_type   age     sex  \\\n1629  HAM_0003602  ISIC_0033004  mel      histo  30.0  female   \n4702  HAM_0000137  ISIC_0027460   nv  follow_up  50.0    male   \n1514  HAM_0007343  ISIC_0033831  mel      histo  70.0    male   \n1865  HAM_0000176  ISIC_0033696  mel      histo  65.0    male   \n4375  HAM_0002369  ISIC_0027016   nv  follow_up  40.0  female   \n\n         localization                                              path  \\\n1629  upper extremity  ../input/ham10000_images_part_2/ISIC_0033004.jpg   \n4702  upper extremity  ../input/ham10000_images_part_1/ISIC_0027460.jpg   \n1514            chest  ../input/ham10000_images_part_2/ISIC_0033831.jpg   \n1865  upper extremity  ../input/ham10000_images_part_2/ISIC_0033696.jpg   \n4375              ear  ../input/ham10000_images_part_1/ISIC_0027016.jpg   \n\n             cell_type  cell_type_idx  \n1629    dermatofibroma              6  \n4702  Melanocytic nevi              4  \n1514    dermatofibroma              6  \n1865    dermatofibroma              6  \n4375  Melanocytic nevi              4  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>lesion_id</th>\n      <th>image_id</th>\n      <th>dx</th>\n      <th>dx_type</th>\n      <th>age</th>\n      <th>sex</th>\n      <th>localization</th>\n      <th>path</th>\n      <th>cell_type</th>\n      <th>cell_type_idx</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>1629</th>\n      <td>HAM_0003602</td>\n      <td>ISIC_0033004</td>\n      <td>mel</td>\n      <td>histo</td>\n      <td>30.0</td>\n      <td>female</td>\n      <td>upper extremity</td>\n      <td>../input/ham10000_images_part_2/ISIC_0033004.jpg</td>\n      <td>dermatofibroma</td>\n      <td>6</td>\n    </tr>\n    <tr>\n      <th>4702</th>\n      <td>HAM_0000137</td>\n      <td>ISIC_0027460</td>\n      <td>nv</td>\n      <td>follow_up</td>\n      <td>50.0</td>\n      <td>male</td>\n      <td>upper extremity</td>\n      <td>../input/ham10000_images_part_1/ISIC_0027460.jpg</td>\n      <td>Melanocytic nevi</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>1514</th>\n      <td>HAM_0007343</td>\n      <td>ISIC_0033831</td>\n      <td>mel</td>\n      <td>histo</td>\n      <td>70.0</td>\n      <td>male</td>\n      <td>chest</td>\n      <td>../input/ham10000_images_part_2/ISIC_0033831.jpg</td>\n      <td>dermatofibroma</td>\n      <td>6</td>\n    </tr>\n    <tr>\n      <th>1865</th>\n      <td>HAM_0000176</td>\n      <td>ISIC_0033696</td>\n      <td>mel</td>\n      <td>histo</td>\n      <td>65.0</td>\n      <td>male</td>\n      <td>upper extremity</td>\n      <td>../input/ham10000_images_part_2/ISIC_0033696.jpg</td>\n      <td>dermatofibroma</td>\n      <td>6</td>\n    </tr>\n    <tr>\n      <th>4375</th>\n      <td>HAM_0002369</td>\n      <td>ISIC_0027016</td>\n      <td>nv</td>\n      <td>follow_up</td>\n      <td>40.0</td>\n      <td>female</td>\n      <td>ear</td>\n      <td>../input/ham10000_images_part_1/ISIC_0027016.jpg</td>\n      <td>Melanocytic nevi</td>\n      <td>4</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# this will tell us how many images are associated with each lesion_id\ndf_undup = df_original.groupby('lesion_id').count()\n# now we filter out lesion_id's that have only one image associated with it\ndf_undup = df_undup[df_undup['image_id'] == 1]\ndf_undup.reset_index(inplace=True)\ndf_undup.head()","execution_count":6,"outputs":[{"output_type":"execute_result","execution_count":6,"data":{"text/plain":"     lesion_id  image_id  dx  dx_type  age  sex  localization  path  \\\n0  HAM_0000001         1   1        1    1    1             1     1   \n1  HAM_0000003         1   1        1    1    1             1     1   \n2  HAM_0000004         1   1        1    1    1             1     1   \n3  HAM_0000007         1   1        1    1    1             1     1   \n4  HAM_0000008         1   1        1    1    1             1     1   \n\n   cell_type  cell_type_idx  \n0          1              1  \n1          1              1  \n2          1              1  \n3          1              1  \n4          1              1  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>lesion_id</th>\n      <th>image_id</th>\n      <th>dx</th>\n      <th>dx_type</th>\n      <th>age</th>\n      <th>sex</th>\n      <th>localization</th>\n      <th>path</th>\n      <th>cell_type</th>\n      <th>cell_type_idx</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>HAM_0000001</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>HAM_0000003</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>HAM_0000004</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>HAM_0000007</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>HAM_0000008</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# here we identify lesion_id's that have duplicate images and those that have only one image.\ndef get_duplicates(x):\n    unique_list = list(df_undup['lesion_id'])\n    if x in unique_list:\n        return 'unduplicated'\n    else:\n        return 'duplicated'\n\n# create a new colum that is a copy of the lesion_id column\ndf_original['duplicates'] = df_original['lesion_id']\n# apply the function to this new column\ndf_original['duplicates'] = df_original['duplicates'].apply(get_duplicates)\ndf_original.sample(5)","execution_count":7,"outputs":[{"output_type":"execute_result","execution_count":7,"data":{"text/plain":"        lesion_id      image_id     dx dx_type   age     sex localization  \\\n9962  HAM_0005228  ISIC_0026702  akiec   histo  80.0  female         face   \n7037  HAM_0006856  ISIC_0028831     nv   histo  35.0    male        chest   \n1485  HAM_0000179  ISIC_0034046    mel   histo  45.0    male         back   \n8747  HAM_0004802  ISIC_0030966     nv   histo  45.0  female         back   \n7934  HAM_0007216  ISIC_0032962     nv   histo  30.0    male         back   \n\n                                                  path          cell_type  \\\n9962  ../input/ham10000_images_part_1/ISIC_0026702.jpg  Actinic keratoses   \n7037  ../input/ham10000_images_part_1/ISIC_0028831.jpg   Melanocytic nevi   \n1485  ../input/ham10000_images_part_2/ISIC_0034046.jpg     dermatofibroma   \n8747  ../input/ham10000_images_part_2/ISIC_0030966.jpg   Melanocytic nevi   \n7934  ../input/ham10000_images_part_2/ISIC_0032962.jpg   Melanocytic nevi   \n\n      cell_type_idx    duplicates  \n9962              0  unduplicated  \n7037              4    duplicated  \n1485              6    duplicated  \n8747              4    duplicated  \n7934              4  unduplicated  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>lesion_id</th>\n      <th>image_id</th>\n      <th>dx</th>\n      <th>dx_type</th>\n      <th>age</th>\n      <th>sex</th>\n      <th>localization</th>\n      <th>path</th>\n      <th>cell_type</th>\n      <th>cell_type_idx</th>\n      <th>duplicates</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>9962</th>\n      <td>HAM_0005228</td>\n      <td>ISIC_0026702</td>\n      <td>akiec</td>\n      <td>histo</td>\n      <td>80.0</td>\n      <td>female</td>\n      <td>face</td>\n      <td>../input/ham10000_images_part_1/ISIC_0026702.jpg</td>\n      <td>Actinic keratoses</td>\n      <td>0</td>\n      <td>unduplicated</td>\n    </tr>\n    <tr>\n      <th>7037</th>\n      <td>HAM_0006856</td>\n      <td>ISIC_0028831</td>\n      <td>nv</td>\n      <td>histo</td>\n      <td>35.0</td>\n      <td>male</td>\n      <td>chest</td>\n      <td>../input/ham10000_images_part_1/ISIC_0028831.jpg</td>\n      <td>Melanocytic nevi</td>\n      <td>4</td>\n      <td>duplicated</td>\n    </tr>\n    <tr>\n      <th>1485</th>\n      <td>HAM_0000179</td>\n      <td>ISIC_0034046</td>\n      <td>mel</td>\n      <td>histo</td>\n      <td>45.0</td>\n      <td>male</td>\n      <td>back</td>\n      <td>../input/ham10000_images_part_2/ISIC_0034046.jpg</td>\n      <td>dermatofibroma</td>\n      <td>6</td>\n      <td>duplicated</td>\n    </tr>\n    <tr>\n      <th>8747</th>\n      <td>HAM_0004802</td>\n      <td>ISIC_0030966</td>\n      <td>nv</td>\n      <td>histo</td>\n      <td>45.0</td>\n      <td>female</td>\n      <td>back</td>\n      <td>../input/ham10000_images_part_2/ISIC_0030966.jpg</td>\n      <td>Melanocytic nevi</td>\n      <td>4</td>\n      <td>duplicated</td>\n    </tr>\n    <tr>\n      <th>7934</th>\n      <td>HAM_0007216</td>\n      <td>ISIC_0032962</td>\n      <td>nv</td>\n      <td>histo</td>\n      <td>30.0</td>\n      <td>male</td>\n      <td>back</td>\n      <td>../input/ham10000_images_part_2/ISIC_0032962.jpg</td>\n      <td>Melanocytic nevi</td>\n      <td>4</td>\n      <td>unduplicated</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_original['duplicates'].value_counts()","execution_count":8,"outputs":[{"output_type":"execute_result","execution_count":8,"data":{"text/plain":"unduplicated    5514\nduplicated      4501\nName: duplicates, dtype: int64"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# now we filter out images that don't have duplicates\ndf_undup = df_original[df_original['duplicates'] == 'unduplicated']\ndf_undup.shape","execution_count":9,"outputs":[{"output_type":"execute_result","execution_count":9,"data":{"text/plain":"(5514, 11)"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# now we create a val set using df because we are sure that none of these images have augmented duplicates in the train set\ny = df_undup['cell_type_idx']\n_, df_val = train_test_split(df_undup, test_size=0.2, random_state=101, stratify=y)\ndf_val.shape","execution_count":10,"outputs":[{"output_type":"execute_result","execution_count":10,"data":{"text/plain":"(1103, 11)"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_val['cell_type_idx'].value_counts()","execution_count":11,"outputs":[{"output_type":"execute_result","execution_count":11,"data":{"text/plain":"4    883\n2     88\n6     46\n1     35\n0     30\n5     13\n3      8\nName: cell_type_idx, dtype: int64"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# This set will be df_original excluding all rows that are in the val set\n# This function identifies if an image is part of the train or val set.\ndef get_val_rows(x):\n    # create a list of all the lesion_id's in the val set\n    val_list = list(df_val['image_id'])\n    if str(x) in val_list:\n        return 'val'\n    else:\n        return 'train'\n\n# identify train and val rows\n# create a new colum that is a copy of the image_id column\ndf_original['train_or_val'] = df_original['image_id']\n# apply the function to this new column\ndf_original['train_or_val'] = df_original['train_or_val'].apply(get_val_rows)\n# filter out train rows\ndf_train = df_original[df_original['train_or_val'] == 'train']\nprint(len(df_train))\nprint(len(df_val))","execution_count":12,"outputs":[{"output_type":"stream","text":"8912\n1103\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train['cell_type_idx'].value_counts()","execution_count":13,"outputs":[{"output_type":"execute_result","execution_count":13,"data":{"text/plain":"4    5822\n6    1067\n2    1011\n1     479\n0     297\n5     129\n3     107\nName: cell_type_idx, dtype: int64"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_val['cell_type'].value_counts()","execution_count":14,"outputs":[{"output_type":"execute_result","execution_count":14,"data":{"text/plain":"Melanocytic nevi                  883\nBenign keratosis-like lesions      88\ndermatofibroma                     46\nBasal cell carcinoma               35\nActinic keratoses                  30\nVascular lesions                   13\nDermatofibroma                      8\nName: cell_type, dtype: int64"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Copy fewer class to balance the number of 7 classes\ndata_aug_rate = [15,10,5,50,0,40,5]\nfor i in range(7):\n    if data_aug_rate[i]:\n        df_train=df_train.append([df_train.loc[df_train['cell_type_idx'] == i,:]]*(data_aug_rate[i]-1), ignore_index=True)\ndf_train['cell_type'].value_counts()","execution_count":15,"outputs":[{"output_type":"execute_result","execution_count":15,"data":{"text/plain":"Melanocytic nevi                  5822\nDermatofibroma                    5350\ndermatofibroma                    5335\nVascular lesions                  5160\nBenign keratosis-like lesions     5055\nBasal cell carcinoma              4790\nActinic keratoses                 4455\nName: cell_type, dtype: int64"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# df_val, df_test = train_test_split(df_val, test_size=0.5)\ndf_train = df_train.reset_index()\ndf_val = df_val.reset_index()\n# df_test = df_test.reset_index()","execution_count":16,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Model building"},{"metadata":{"trusted":true},"cell_type":"code","source":"# feature_extract is a boolean that defines if we are finetuning or feature extracting. \n# If feature_extract = False, the model is finetuned and all model parameters are updated. \n# If feature_extract = True, only the last layer parameters are updated, the others remain fixed.\ndef set_parameter_requires_grad(model, feature_extracting):\n    if feature_extracting:\n        for param in model.parameters():\n            param.requires_grad = False","execution_count":17,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def initialize_model(num_classes, feature_extract, use_pretrained=True):\n    # Initialize these variables which will be set in this if statement. Each of these\n    #   variables is model specific.\n    model_ft = None\n    input_size = 0\n\n    model_ft = models.squeezenet1_0(pretrained=use_pretrained)\n    set_parameter_requires_grad(model_ft, feature_extract)\n    model_ft.classifier[1] = nn.Conv2d(512, num_classes, kernel_size=(1,1), stride=(1,1))\n    model_ft.num_classes = num_classes\n    input_size = 224\n\n    return model_ft, input_size","execution_count":18,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"You can change your backbone network, here are 4 different networks, each network also has sevaral versions. Considering the limited training data, we used the ImageNet pre-training model for fine-tuning. This can speed up the convergence of the model and improve the accuracy.\n\nThere is one thing you need to pay attention to, the input size of Inception is different from the others (299x299), you need to change the setting of compute_img_mean_std() function "},{"metadata":{"trusted":true},"cell_type":"code","source":"# resnet,vgg,densenet,inception\n\nnum_classes = 7\nfeature_extract = False\n# Initialize the model for this run\nmodel_ft, input_size = initialize_model(num_classes, feature_extract, use_pretrained=True)\n# Define the device:\ndevice = torch.device('cuda:0')\n# Put the model on the device:\nmodel = model_ft.to(device)","execution_count":20,"outputs":[{"output_type":"stream","text":"Downloading: \"https://download.pytorch.org/models/squeezenet1_0-a815701f.pth\" to /tmp/.cache/torch/checkpoints/squeezenet1_0-a815701f.pth\n100%|██████████| 4.79M/4.79M [00:01<00:00, 3.27MB/s]\n","name":"stderr"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# norm_mean = (0.49139968, 0.48215827, 0.44653124)\n# norm_std = (0.24703233, 0.24348505, 0.26158768)\n# define the transformation of the train images.\ntrain_transform = transforms.Compose([transforms.Resize((input_size,input_size)),transforms.RandomHorizontalFlip(),\n                                      transforms.RandomVerticalFlip(),transforms.RandomRotation(20),\n                                      transforms.ColorJitter(brightness=0.1, contrast=0.1, hue=0.1),\n                                        transforms.ToTensor(), transforms.Normalize(norm_mean, norm_std)])\n# define the transformation of the val images.\nval_transform = transforms.Compose([transforms.Resize((input_size,input_size)), transforms.ToTensor(),\n                                    transforms.Normalize(norm_mean, norm_std)])","execution_count":34,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Define a pytorch dataloader for this dataset\nclass HAM10000(Dataset):\n    def __init__(self, df, transform=None):\n        self.df = df\n        self.transform = transform\n\n    def __len__(self):\n        return len(self.df)\n\n    def __getitem__(self, index):\n        # Load data and get label\n        X = Image.open(self.df['path'][index])\n        y = torch.tensor(int(self.df['cell_type_idx'][index]))\n\n        if self.transform:\n            X = self.transform(X)\n\n        return X, y","execution_count":35,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Define the training set using the table train_df and using our defined transitions (train_transform)\ntraining_set = HAM10000(df_train, transform=train_transform)\ntrain_loader = DataLoader(training_set, batch_size=32, shuffle=True, num_workers=4)\n# Same for the validation set:\nvalidation_set = HAM10000(df_val, transform=train_transform)\nval_loader = DataLoader(validation_set, batch_size=32, shuffle=False, num_workers=4)","execution_count":36,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# we use Adam optimizer, use cross entropy loss as our loss function\noptimizer = optim.Adam(model.parameters(), lr=1e-3)\ncriterion = nn.CrossEntropyLoss().to(device)","execution_count":37,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Step 3. Model training"},{"metadata":{"trusted":true},"cell_type":"code","source":"# this function is used during training process, to calculation the loss and accuracy\nclass AverageMeter(object):\n    def __init__(self):\n        self.reset()\n\n    def reset(self):\n        self.val = 0\n        self.avg = 0\n        self.sum = 0\n        self.count = 0\n\n    def update(self, val, n=1):\n        self.val = val\n        self.sum += val * n\n        self.count += n\n        self.avg = self.sum / self.count","execution_count":38,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"total_loss_train, total_acc_train = [],[]\ndef train(train_loader, model, criterion, optimizer, epoch):\n    model.train()\n    train_loss = AverageMeter()\n    train_acc = AverageMeter()\n    curr_iter = (epoch - 1) * len(train_loader)\n    for i, data in enumerate(train_loader):\n        images, labels = data\n        N = images.size(0)\n        # print('image shape:',images.size(0), 'label shape',labels.size(0))\n        images = Variable(images).to(device)\n        labels = Variable(labels).to(device)\n\n        optimizer.zero_grad()\n        outputs = model(images)\n\n        loss = criterion(outputs, labels)\n        loss.backward()\n        optimizer.step()\n        prediction = outputs.max(1, keepdim=True)[1]\n        train_acc.update(prediction.eq(labels.view_as(prediction)).sum().item()/N)\n        train_loss.update(loss.item())\n        curr_iter += 1\n        if (i + 1) % 100 == 0:\n            print('[epoch %d], [iter %d / %d], [train loss %.5f], [train acc %.5f]' % (\n                epoch, i + 1, len(train_loader), train_loss.avg, train_acc.avg))\n            total_loss_train.append(train_loss.avg)\n            total_acc_train.append(train_acc.avg)\n    return train_loss.avg, train_acc.avg","execution_count":39,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def validate(val_loader, model, criterion, optimizer, epoch):\n    model.eval()\n    val_loss = AverageMeter()\n    val_acc = AverageMeter()\n    with torch.no_grad():\n        for i, data in enumerate(val_loader):\n            images, labels = data\n            N = images.size(0)\n            images = Variable(images).to(device)\n            labels = Variable(labels).to(device)\n\n            outputs = model(images)\n            prediction = outputs.max(1, keepdim=True)[1]\n\n            val_acc.update(prediction.eq(labels.view_as(prediction)).sum().item()/N)\n\n            val_loss.update(criterion(outputs, labels).item())\n\n    print('------------------------------------------------------------')\n    print('[epoch %d], [val loss %.5f], [val acc %.5f]' % (epoch, val_loss.avg, val_acc.avg))\n    print('------------------------------------------------------------')\n    return val_loss.avg, val_acc.avg","execution_count":40,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"epoch_num = 10\nbest_val_acc = 0\ntotal_loss_val, total_acc_val = [],[]\nfor epoch in range(1, epoch_num+1):\n    loss_train, acc_train = train(train_loader, model, criterion, optimizer, epoch)\n    loss_val, acc_val = validate(val_loader, model, criterion, optimizer, epoch)\n    total_loss_val.append(loss_val)\n    total_acc_val.append(acc_val)\n    if acc_val > best_val_acc:\n        best_val_acc = acc_val\n        print('*****************************************************')\n        print('best record: [epoch %d], [val loss %.5f], [val acc %.5f]' % (epoch, loss_val, acc_val))\n        print('*****************************************************')","execution_count":null,"outputs":[{"output_type":"stream","text":"[epoch 1], [iter 100 / 1124], [train loss 1.95045], [train acc 0.15563]\n[epoch 1], [iter 200 / 1124], [train loss 1.91136], [train acc 0.18672]\n[epoch 1], [iter 300 / 1124], [train loss 1.87628], [train acc 0.21563]\n[epoch 1], [iter 400 / 1124], [train loss 1.84051], [train acc 0.23961]\n[epoch 1], [iter 500 / 1124], [train loss 1.80876], [train acc 0.25588]\n[epoch 1], [iter 600 / 1124], [train loss 1.78318], [train acc 0.27078]\n[epoch 1], [iter 700 / 1124], [train loss 1.75758], [train acc 0.28446]\n[epoch 1], [iter 800 / 1124], [train loss 1.73534], [train acc 0.29688]\n[epoch 1], [iter 900 / 1124], [train loss 1.71554], [train acc 0.30694]\n[epoch 1], [iter 1000 / 1124], [train loss 1.70379], [train acc 0.31303]\n[epoch 1], [iter 1100 / 1124], [train loss 1.69003], [train acc 0.32097]\n------------------------------------------------------------\n[epoch 1], [val loss 0.73150], [val acc 0.74786]\n------------------------------------------------------------\n*****************************************************\nbest record: [epoch 1], [val loss 0.73150], [val acc 0.74786]\n*****************************************************\n[epoch 2], [iter 100 / 1124], [train loss 1.52241], [train acc 0.39594]\n[epoch 2], [iter 200 / 1124], [train loss 1.52065], [train acc 0.39797]\n[epoch 2], [iter 300 / 1124], [train loss 1.51994], [train acc 0.40417]\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"## Model evaluation"},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = plt.figure(num = 2)\nfig1 = fig.add_subplot(2,1,1)\nfig2 = fig.add_subplot(2,1,2)\nfig1.plot(total_loss_train, label = 'training loss')\nfig1.plot(total_acc_train, label = 'training accuracy')\nfig2.plot(total_loss_val, label = 'validation loss')\nfig2.plot(total_acc_val, label = 'validation accuracy')\nplt.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def plot_confusion_matrix(cm, classes,\n                          normalize=False,\n                          title='Confusion matrix',\n                          cmap=plt.cm.Blues):\n    \"\"\"\n    This function prints and plots the confusion matrix.\n    Normalization can be applied by setting `normalize=True`.\n    \"\"\"\n    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n    plt.title(title)\n    plt.colorbar()\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes, rotation=45)\n    plt.yticks(tick_marks, classes)\n\n    if normalize:\n        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n\n    thresh = cm.max() / 2.\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        plt.text(j, i, cm[i, j],\n                 horizontalalignment=\"center\",\n                 color=\"white\" if cm[i, j] > thresh else \"black\")\n\n    plt.tight_layout()\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.eval()\ny_label = []\ny_predict = []\nwith torch.no_grad():\n    for i, data in enumerate(val_loader):\n        images, labels = data\n        N = images.size(0)\n        images = Variable(images).to(device)\n        outputs = model(images)\n        prediction = outputs.max(1, keepdim=True)[1]\n        y_label.extend(labels.cpu().numpy())\n        y_predict.extend(np.squeeze(prediction.cpu().numpy().T))\n\n# compute the confusion matrix\nconfusion_mtx = confusion_matrix(y_label, y_predict)\n# plot the confusion matrix\nplot_labels = ['akiec', 'bcc', 'bkl', 'df', 'nv', 'vasc','mel']\nplot_confusion_matrix(confusion_mtx, plot_labels)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Generate a classification report\nreport = classification_report(y_label, y_predict, target_names=plot_labels)\nprint(report)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"label_frac_error = 1 - np.diag(confusion_mtx) / np.sum(confusion_mtx, axis=1)\nplt.bar(np.arange(7),label_frac_error)\nplt.xlabel('True Label')\nplt.ylabel('Fraction classified incorrectly')","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}